# -*- coding: utf-8 -*-
"""task1_32973381.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aCaOZAOt2FruZIrjRd7FEZtf1Ow_wxy0

<div class="alert alert-block alert-success">
    
# FIT5196 Task 1 in Assessment 1
#### Student Name: Tsz Yan CHUNG
#### Student ID: 32973381

Date: 19/08/2022


Environment: Google colab

Libraries used:
* os (for interacting with the operating system, mainly for accessing files in different directories) 
* re (for regular expression, installed and imported) 
* pandas (for data manipulation) 
    
</div>

<div class="alert alert-block alert-danger">
    
## Table of Contents

</div>    

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Review Files](#examine) <br>
[4. Loading and Parsing Files](#parse) <br>
$\;\;\;\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>
$\;\;\;\;$[4.2. Reading Files](#Read) <br>
$\;\;\;\;$[4.3. Whatever else](#latin) <br>
[5. Writing to an CSV File](#write) <br>
$\;\;\;\;$[5.1. Verification - using the sample files](#test_xml) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

-------------------------------------

<div class="alert alert-block alert-warning">

## 1.  Introduction  <a class="anchor" name="Intro"></a>
    
</div>

This assessment regards extracting data from semi-sctuctured text files. The dataset contained 500 `.txt` files which included various information about user reviews. In particular, there are four types of attributes that are desired: product ID, product review, review summary and the latest review dates.

The following notebook will go through all the processes step by step, until the aim to create a CSV following the sample.csv format is achieved.

-------------------------------------

<div class="alert alert-block alert-warning">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>
 </div>

The only permitted packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:

* **os:** to interact with the operating system, e.g. navigate through folders
* **re:** to define and use regular expressions
* **pandas:** to create, access and manipulate dataframe
"""

# Import libraries
import pandas as pd
import os
import re

# This task was conducted on Google colab
from google.colab import drive
drive.mount('/content/drive')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 3.  Examining and Loading Files<a class="anchor" name="examine"></a>

 </div>

Before diving into the details of the files directly,
it is important to understand the overall structure of the requested output.
"""

os.chdir("/content/drive/Shareddrives/FIT5196-s2-2022/A1/Task1/sample IO/")
sample = pd.read_csv("sample_output_task1.csv", encoding = "UTF-8")

# Display sample
sample

"""On top of this, I also opened a few text file manually and noticed the name of the desired information might varies slightly, and they are enclosed by certain patterns like the h1/ h2 quotations.

In the following stpes, after extracting all of the data from all text files to a python variable, abnormal formats will also be observed.

"""

# Set up directory
os.chdir("/content/drive/Shareddrives/FIT5196-s2-2022/A1/Task1/input_data")

# Access all txt files in the directory
data = ''
for root, dirs, files in os.walk("32973381"):
  for file in files:  # remove index after test
    with open("32973381/" + file, "r", encoding = "UTF-8") as f:
      data += f.read()

# Look for all possible combination of <h1> context which indicates specific attributes
attribute_h1 = set(re.findall(r'<h1> (.*?) <h1>', data))
# Look for all possible combination of <h2> context which indicates specific attributes
attribute_h2 = set(re.findall(r'<h2> (.*?) <\/h2>', data))

print('h1:', attribute_h1)
print('h2:',attribute_h2)

"""For the two regular expression methods set up on the cell above:
```
attribute_h1 = set(re.findall(r'<h1> (.*?) <h1>', data))  #method1
attribute_h2 = set(re.findall(r'<h2> (.*?) </h2>', data))  #method2
```
Method1 tries to capture texts surrounded by the h1 quotation among text files, 

while method2 is capturing texts surrounded by the h2 quotation.

By making the captured values a set, the following observations are made:

PID: "PRODUCT_ID", "Product ID", "PRODUCT.ID" <br>
review_summary: "review.summary", "REVIEW SUMMARY" <br>
review_date: "REVIEW DATE", "review.date"<br>
product_review: "product_reviews", "product.reviews"<br>

Regular expression methods can be further set up looking for these specific values

-------------------------------------

<div class="alert alert-block alert-warning"> 

## 4.  Parsing Files <a class="anchor" name="parse"></a>

</div>

After knowing the possible combination of the desired attributes, regular expression methods could be set up one by one for each of these attirbutes.

-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.1. Defining Regular Expressions <a class="anchor" name="Reg_Exp"></a>

Defining correct regular expressions is crucial in extracting desired information from the text efficiently:
<h4>PRODUCT ID</h4>


```
# Get product ID
prod_id = re.findall(r'<h1> .*?[._ ][Ii][Dd] <h1>\n <p> (.*?) <p>', data)
```
First, for Product ID, it is known that each text file represents only one product, and one product ID. By search for text followed by the header "ProductID" and its variations, PID for each text files could be obtained.

For the 3 variations:
```
<h1>["Product ID", "PRODUCT_ID", "PRODUCT.ID"]<h1>
 <p> [PID] <p>
```
I focused on the h1 quotation instead of the word "Product" itselfs, knowing that h1 usually represent header or title, which is relatively rare in a html file. And the rest of the format could be captured by the 3 character groups which allow all 3 variation to fit in, and hence capture the PID on the following line.

-------------------------------------

<h4>PRODUCT REVIEW & PRODUCT SUMMARY</h4>


```
# Get product review
prod_text = re.findall(r"(?:(?:<h2>\sREVIEW\sTEXT\s<\/h2>)|(?:<h2>\sreview\.text\s>\/h2>))\n\n\s<p>\s?(.+\.?)\s<\/p>", data)
```
For product review, by strictly following the sample output provided, which is representing review_text in the text files.

To capture such texts, knowing that the values are strictly followed by the h2 heading "review text" and its variation, non-capturing groups are set up to allow different format of text in the regular expression. The white spaces and new lines are also observed and applied to the regular expression.

Which the same logic could be applied to product summary as well.

```
# Get product summary
review_summary = re.findall(r"(?:(?:<h2>\sREVIEW\sSUMMARY\s<\/h2>)|(?:<h2>\sreview\.summary\s>\/h2>))\n\n\s<p>\s?(.+\.?)\s<\/p>", data)
```

-------------------------------------

<h4>DATES</h4>


```
# Get all dates
date = re.findall(r"(?:(?:<h2>\sREVIEW\sDATE\s<\/h2>)|(?:<h2>\sreview[.]date\s<\/h2>))\n\s+(\d{1,2}-\d{1,2}-\d{4})", data)
```
For dates data, knowing that they are following the "review date" text in h2 quotation, specifying 2 consecutive one or two digits number and one 4 digits number for the year, join with forward slash could help the regular expression works.

-------------------------------------

These patterns are used in the next step when reading the files.

<div class="alert alert-block alert-info">
    
### 4.2. Reading Files <a class="anchor" name="Read"></a>

When all regular expression methods are defined, they can be applied to the actual text files. To extract information from each text files, files will be went through one by one, storing the desired values into a separated list, so that they are stored in order and can be easily retrieved.
"""

# Define variables
df = ''
data = ''
prod_text = []
review_sum = []
latest_review_date = []
prod_id = []

# Loop through 500 text files to get all the information needed
# data of each attribute will be saved into a list as a single values. 
# So each list of an attribute should have a length the same as the total number of files read in the end 
for root, dirs, files in os.walk("32973381"):  
  for file in files[:]:    # Indexing are added for testing purpose only, remove after debug
    with open("32973381/" + file, "r", encoding = "UTF-8") as f:
      data = f.read()
      # Extract desired data into lists one by one:
      prod_text.append(str(re.findall(r"(?:(?:<h2>\sREVIEW\sTEXT\s<\/h2>)|(?:<h2>\sreview\.text\s<\/h2>))\n\n\s<p>\s?(.+\.?)\s<\/p>", data)))
      review_sum.append(str(re.findall(r"(?:(?:<h2>\sREVIEW\sSUMMARY\s<\/h2>)|(?:<h2>\sreview\.summary\s<\/h2>))\n\n\s<p>\s?(.+\.?)\s<\/p>", data)))
      prod_id.append(re.findall(r"<h1> .*?[._ ][Ii][Dd] <h1>\n <p> (.*?) <p>", data)[0])
      # Wrangle date data and get latest review date
      date = re.findall(r"(?:(?:<h2>\sREVIEW\sDATE\s<\/h2>)|(?:<h2>\sreview[.]date\s<\/h2>))\n\s+(\d{1,2}-\d{1,2}-\d{4})", data)
      date_list = []  # Create empty date_list to store date value for every loop, as only the latest date is required
      for line in date:  # To find the latest review date, convert each date into int format "YYYYMMDD", the latest date coud be obtained by sorting dates in order
        date_list.append(int(str(line[6:]) + str(line[0:2]) + str(line[3:5])))
      review_date = (sorted(date_list, reverse = True)[0])
      # Changing date into the same format as shwon in the sample csv file
      latest_review_date.append([str(review_date)[6:] + '/' + str(review_date)[4:6] + '/' + str(review_date)[:4]][0])

"""Some extra works were done on the dates data. As it is only required to derive the latest review date. Therefore from all the dates info extracted for a product, they are re-formatted as integer "YYYYMMDD", which they could now be compared. 

After obtaining the latest review date (largest value), it is stored in latest_review_date list.

-------------------------------------

Let's take a look at the extracted data, they should have the same length. 

We can see that ids, dates, revieiws summary and reviews text are parsed and stored correctly.
"""

# Checking length of extracted lists
len(prod_text) == len(prod_id) == len(review_sum) == len(latest_review_date)

"""<div class="alert alert-block alert-info">
    
### 4.3. Convert Extracted Data into DataFrame <a class="anchor" name="df"></a>
When all data are extracted successfully, they could be assigned to a dictionary. As the lists created from the loop are in order, if there are no misisng values, the DataFrame is correctly built.
"""

# Create dataframe
df = pd.DataFrame({"PID":prod_id, "product_review":prod_text, "review_summary":review_sum, "latest_review_date":latest_review_date})

# Check for missing values
df.sum().isna()

"""-------------------------------------

<div class="alert alert-block alert-warning"> 

## 5.  Writing to an CSV File <a class="anchor" name="write"></a>

</div>

As the dataframe is created, it can now be outputed as a csv file using one of the pandas method: to_csv.
"""

# Export dataframe as a csv file to mydrive
# Set up directory
os.chdir("/content/drive/MyDrive/")

# Writing file as csv, neglect index when exporting dataframe
df.to_csv("32973381.csv", index = False, mode = "w", encoding = "UTF-8")

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 5.1. Verification of the Generated CSV File <a class="anchor" name="test_xml"></a>

To verify our output, the csv can be read again using the read_csv metho, to check all the data inside
"""

# Set up directory
os.chdir("/content/drive/MyDrive/")

# Read csv
df_read = pd.read_csv("32973381.csv", encoding = "UTF-8")

# Display dataframe
df_read

"""Check for data loss when reading csv file"""

# check for missing values
df_read.sum().isna()

"""Check if data is consistent to the original dataframe"""

df_read == df

"""-------------------------------------

<div class="alert alert-block alert-warning"> 

## 6. Summary <a class="anchor" name="summary"></a>

</div>

From all the process, it can be told that with the correct regular expression method set up, and parsing properly, data could be extracted quickly.

It is also worth mentioning that due to the word limit of a Microsoft excel's cell (limit of 32759 characters), the csv output to an excel file may not be 100% accurate. The word limits setting may cause error to the file structure.

If the csv file will be read directly using the pandas library or other data processing applications, then it should not be a problem.

However, if Microsoft excel is a desired format of output for some reason, the data might need to be parsed again, one of the suggestion is as follow:

```
for idx, value in zip(df.index, df["product_review"]):
  if len(df["product_review"][idx]) > 32758:
    df["product_review"][idx] = df["product_review"][idx][:32758]
```

Although some information could be loss as this method shorten the values in a cell in order to fit into a cell, this might be useful if certain degree of loss of data is acceptable.

-------------------------------------

<div class="alert alert-block alert-warning"> 

## 7. References <a class="anchor" name="Ref"></a>

</div>

[1]<a class="anchor" name="ref-2"></a> Pandas documentation on DataFrame method,https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html, Accessed on 23/08/2022

[2]<a class="anchor" name="ref-2"></a> Pandas documentation on DataFrame.to_csv method, https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html, Accessed on 23/08/2022

[3] Stack Overflow discussion on how to match all characters with regular expression, https://stackoverflow.com/questions/2912894/how-to-match-any-character-in-regular-expression, Accessed on 23/08/2022

[4] Regular expression cheat sheet on Geeksforgeeks, https://www.geeksforgeeks.org/python-regex-cheat-sheet/, Accessed on 23/08/2022

[5] Regex tester. https://regex101.com/. Accessed on 22/08/2022

## --------------------------------------------------------------------------------------------------------------------------
"""